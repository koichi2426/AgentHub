#!/usr/bin/env python3
"""
train_and_export.py

ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€INT8é‡å­åŒ–ã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Ÿè¡Œç’°å¢ƒã§éå¯¾è©±çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
"""

from __future__ import annotations
import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType
import numpy as np
from tqdm import tqdm
import sys # çµ‚äº†å‡¦ç†ç”¨

# ==========================
# å…±é€šè¨­å®š (å¼•æ•°ã§ä¸Šæ›¸ãã•ã‚Œãªã„é™ã‚Šã“ã®å€¤ã‚’ä½¿ç”¨)
# ==========================
MAX_LENGTH = 32
BATCH_SIZE = 16
EPOCHS = 3 # å®Ÿè¡Œæ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’3ã«è¨­å®š
LR = 2e-5
# Dockerç’°å¢ƒã§ã¯CUDAãŒåˆ©ç”¨å¯èƒ½ã‹ä¸æ˜ãªãŸã‚ã€CPUã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã€CUDAãŒã‚ã‚Œã°ä½¿ç”¨
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TRAIN_DATA_DEFAULT = "data/train_triplets.txt"


# ==========================
# SBERTæ§‹é€ ï¼ˆmean poolingï¼‰
# ==========================
class SBERTEncoder(nn.Module):
    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model

    def forward(self, input_ids, attention_mask):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden = output.last_hidden_state
        mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
        summed = (last_hidden * mask).sum(1)
        counts = mask.sum(1)
        mean_pooled = summed / counts
        return mean_pooled


# ==========================
# Tripletãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
# ==========================
class TripletDataset(Dataset):
    def __init__(self, path: str, tokenizer):
        self.samples = []
        try:
            with open(path, encoding="utf-8") as f:
                for line in f:
                    parts = line.strip().split("\t")
                    if len(parts) == 3:
                        self.samples.append(tuple(parts))
        except FileNotFoundError:
             print(f"ERROR: Training data file not found at {path}", file=sys.stderr)
             raise
        except Exception as e:
             print(f"ERROR: Failed to read training data: {e}", file=sys.stderr)
             raise

        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        a, p, n = self.samples[idx]
        return self.tokenizer(
            [a, p, n],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=MAX_LENGTH
        )


# ==========================
# Triplet Loss
# ==========================
def triplet_loss(anchor, positive, negative, margin=1.0):
    d_ap = (anchor - positive).pow(2).sum(1)
    d_an = (anchor - negative).pow(2).sum(1)
    return torch.relu(d_ap - d_an + margin).mean()


# ==========================
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†
# ==========================
def finetune_model(model_name_or_path: str, training_file: str, output_dir: str, epochs: int, lr: float):
    print(f"\n[1] Loading model from {model_name_or_path} and tokenizer...")
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã¿
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    base_model = AutoModel.from_pretrained(model_name_or_path)
    model = SBERTEncoder(base_model).to(DEVICE)

    dataset = TripletDataset(training_file, tokenizer)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    print(f"[2] Starting fine-tuning (Epochs: {epochs}, LR: {lr})...")
    model.train()
    for epoch in range(epochs):
        losses = []
        for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
            input_ids = batch["input_ids"].squeeze(1).to(DEVICE)
            attention_mask = batch["attention_mask"].squeeze(1).to(DEVICE)
            a, p, n = input_ids[:, 0, :], input_ids[:, 1, :], input_ids[:, 2, :]
            am, pm, nm = attention_mask[:, 0, :], attention_mask[:, 1, :], attention_mask[:, 2, :]
            va, vp, vn = model(a, am), model(p, pm), model(n, nm)
            loss = triplet_loss(va, vp, vn)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            losses.append(loss.item())

        print(f"  âœ… Epoch {epoch+1}/{epochs} Average Loss: {np.mean(losses):.4f}")

    print("\nâœ… Fine-tuning complete.")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    model.bert.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    torch.save(model.state_dict(), os.path.join(output_dir, "pytorch_model.bin"))
    print(f"âœ… Model saved to {output_dir}")
    return tokenizer, model.bert # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®ãŸã‚ã€å…ƒã®BERTãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™


# ==========================
# ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# ==========================
def export_onnx(model, tokenizer, output_dir: str):
    print("\n[3] Exporting ONNX (FP32)...")
    model.eval()
    # output_dirã«ä¿å­˜ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®š
    onnx_fp32 = os.path.join(output_dir, "model_fp32.onnx")
    dummy_input_ids = torch.randint(0, tokenizer.vocab_size, (1, MAX_LENGTH), dtype=torch.long)
    dummy_attention_mask = torch.ones((1, MAX_LENGTH), dtype=torch.long)

    # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¯å…ƒã®BERTãƒ¢ãƒ‡ãƒ«ã§è¡Œã† (SBERTEncoderã§ã¯ãªã„)
    torch.onnx.export(
        model,
        (dummy_input_ids, dummy_attention_mask),
        onnx_fp32,
        input_names=["input_ids", "attention_mask"],
        output_names=["last_hidden_state"], # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ã‚’å‡ºåŠ›
        dynamic_axes={"input_ids": {0: "batch"}, "attention_mask": {0: "batch"}},
        opset_version=13
    )
    print(f"âœ… ONNX FP32 output complete â†’ {onnx_fp32}")
    return onnx_fp32


# ==========================
# ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
# ==========================
class DummyCalibReader(CalibrationDataReader):
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¨å¥¨ï¼‰
        self.datas = [
            tokenizer(
                "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œã¯æ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚",
                return_tensors="np",
                padding="max_length",
                truncation=True,
                max_length=MAX_LENGTH
            )
        ]
        self.index = 0

    def get_next(self):
        if self.index < len(self.datas):
            data = self.datas[self.index]
            self.index += 1
            return {
                "input_ids": data["input_ids"],
                "attention_mask": data["attention_mask"]
            }
        return None


# ==========================
# é‡å­åŒ–
# ==========================
def quantize_model(tokenizer, onnx_fp32: str, output_dir: str):
    print("\n[4] Executing INT8 quantization...")
    onnx_int8 = os.path.join(output_dir, "model_int8.onnx")
    quantize_static(
        model_input=onnx_fp32,
        model_output=onnx_int8,
        calibration_data_reader=DummyCalibReader(tokenizer),
        quant_format=QuantType.QUInt8
    )
    print(f"âœ… INT8 quantization complete â†’ {onnx_int8}")


# ==========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç† (éå¯¾è©±å‹)
# ==========================
def main():
    parser = argparse.ArgumentParser(description="Fine-tuning and model export script for TinyBERT models.")
    parser.add_argument("--base_model_path", required=True, help="Local path to the base model directory (e.g., /app/worker/.../bert-tiny).")
    parser.add_argument("--training_file", required=True, help="Local path to the training data file (e.g., /tmp/job_ID/data/train_triplets.txt).")
    parser.add_argument("--output_dir", required=True, help="Directory to save the fine-tuned model and exports.")
    parser.add_argument("--epochs", type=int, default=EPOCHS, help=f"Number of training epochs (default: {EPOCHS}).")
    parser.add_argument("--lr", type=float, default=LR, help=f"Learning rate (default: {LR}).")

    args = parser.parse_args()

    # --- å‰å‡¦ç† ---
    if not os.path.exists(args.training_file):
        raise FileNotFoundError(f"Training file not found: {args.training_file}")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    os.makedirs(args.output_dir, exist_ok=True)

    # --- è¨“ç·´å®Ÿè¡Œ ---
    tokenizer, model = finetune_model(
        model_name_or_path=args.base_model_path,
        training_file=args.training_file,
        output_dir=args.output_dir,
        epochs=args.epochs,
        lr=args.lr
    )

    # --- ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¨é‡å­åŒ– ---
    onnx_fp32 = export_onnx(model, tokenizer, args.output_dir)
    quantize_model(tokenizer, onnx_fp32, args.output_dir)

    print("\nğŸ¯ All training and export processes completed.")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nFATAL: Training pipeline failed: {e}", file=sys.stderr)
        sys.exit(1) # ãƒ¯ãƒ¼ã‚«ãƒ¼ã«å¤±æ•—ã‚’é€šçŸ¥
