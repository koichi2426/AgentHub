#!/usr/bin/env python3
"""
train_and_export.py

ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€INT8é‡å­åŒ–ã€GGUFã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Ÿè¡Œç’°å¢ƒã§éå¯¾è©±çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
"""

from __future__ import annotations
import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType
import numpy as np
from tqdm import tqdm
import sys # çµ‚äº†å‡¦ç†ç”¨
import subprocess # ğŸš¨ GGUFã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®ãŸã‚ã«è¿½åŠ 

# ==========================
# å…±é€šè¨­å®š (å¼•æ•°ã§ä¸Šæ›¸ãã•ã‚Œãªã„é™ã‚Šã“ã®å€¤ã‚’ä½¿ç”¨)
# ==========================
MAX_LENGTH = 32
BATCH_SIZE = 16
EPOCHS = 3 # å®Ÿè¡Œæ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’3ã«è¨­å®š
LR = 2e-5
# Dockerç’°å¢ƒã§ã¯CUDAãŒåˆ©ç”¨å¯èƒ½ã‹ä¸æ˜ãªãŸã‚ã€CPUã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã€CUDAãŒã‚ã‚Œã°ä½¿ç”¨
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TRAIN_DATA_DEFAULT = "data/train_triplets.txt"


# ==========================
# SBERTæ§‹é€ ï¼ˆmean poolingï¼‰
# ==========================
class SBERTEncoder(nn.Module):
    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model

    def forward(self, input_ids, attention_mask):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden = output.last_hidden_state
        mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
        summed = (last_hidden * mask).sum(1)
        counts = mask.sum(1)
        mean_pooled = summed / counts
        return mean_pooled


# ==========================
# Tripletãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
# ==========================
class TripletDataset(Dataset):
    def __init__(self, path: str, tokenizer):
        self.samples = []
        try:
            with open(path, encoding="utf-8") as f:
                for line in f:
                    parts = line.strip().split("\t")
                    if len(parts) == 3:
                        self.samples.append(tuple(parts))
        except FileNotFoundError:
             print(f"ERROR: Training data file not found at {path}", file=sys.stderr)
             raise
        except Exception as e:
             print(f"ERROR: Failed to read training data: {e}", file=sys.stderr)
             raise

        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        a, p, n = self.samples[idx]
        return self.tokenizer(
            [a, p, n],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=MAX_LENGTH
        )


# ==========================
# Triplet Loss
# ==========================
def triplet_loss(anchor, positive, negative, margin=1.0):
    d_ap = (anchor - positive).pow(2).sum(1)
    d_an = (anchor - negative).pow(2).sum(1)
    return torch.relu(d_ap - d_an + margin).mean()


# ==========================
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†
# ==========================
def finetune_model(model_name_or_path: str, training_file: str, output_dir: str, epochs: int, lr: float):
    print(f"\n[1] Loading model from {model_name_or_path} and tokenizer...")
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‹ã‚‰èª­ã¿è¾¼ã¿
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    base_model = AutoModel.from_pretrained(model_name_or_path)
    model = SBERTEncoder(base_model).to(DEVICE)

    dataset = TripletDataset(training_file, tokenizer)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    print(f"[2] Starting fine-tuning (Epochs: {epochs}, LR: {lr})...")
    model.train()
    for epoch in range(epochs):
        losses = []
        for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
            input_ids = batch["input_ids"].squeeze(1).to(DEVICE)
            attention_mask = batch["attention_mask"].squeeze(1).to(DEVICE)
            a, p, n = input_ids[:, 0, :], input_ids[:, 1, :], input_ids[:, 2, :]
            am, pm, nm = attention_mask[:, 0, :], attention_mask[:, 1, :], attention_mask[:, 2, :]
            va, vp, vn = model(a, am), model(p, pm), model(n, nm)
            loss = triplet_loss(va, vp, vn)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            losses.append(loss.item())

        print(f"  âœ… Epoch {epoch+1}/{epochs} Average Loss: {np.mean(losses):.4f}")

    print("\nâœ… Fine-tuning complete.")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    model.bert.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    torch.save(model.state_dict(), os.path.join(output_dir, "pytorch_model.bin"))
    print(f"âœ… Model saved to {output_dir}")
    return tokenizer, model.bert # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®ãŸã‚ã€å…ƒã®BERTãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™


# ==========================
# ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# ==========================
def export_onnx(model, tokenizer, output_dir: str):
    print("\n[3] Exporting ONNX (FP32)...")
    model.eval()
    # output_dirã«ä¿å­˜ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®š
    onnx_fp32 = os.path.join(output_dir, "model_fp32.onnx")
    dummy_input_ids = torch.randint(0, tokenizer.vocab_size, (1, MAX_LENGTH), dtype=torch.long)
    dummy_attention_mask = torch.ones((1, MAX_LENGTH), dtype=torch.long)

    # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¯å…ƒã®BERTãƒ¢ãƒ‡ãƒ«ã§è¡Œã† (SBERTEncoderã§ã¯ãªã„)
    torch.onnx.export(
        model,
        (dummy_input_ids, dummy_attention_mask),
        onnx_fp32,
        input_names=["input_ids", "attention_mask"],
        output_names=["last_hidden_state"], # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ã‚’å‡ºåŠ›
        dynamic_axes={"input_ids": {0: "batch"}, "attention_mask": {0: "batch"}},
        opset_version=13
    )
    print(f"âœ… ONNX FP32 output complete â†’ {onnx_fp32}")
    return onnx_fp32


# ==========================
# ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
# ==========================
class DummyCalibReader(CalibrationDataReader):
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆå®Ÿéš›ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¨å¥¨ï¼‰
        self.datas = [
            tokenizer(
                "ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œã¯æ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚",
                return_tensors="np",
                padding="max_length",
                truncation=True,
                max_length=MAX_LENGTH
            )
        ]
        self.index = 0

    def get_next(self):
        if self.index < len(self.datas):
            data = self.datas[self.index]
            self.index += 1
            return {
                "input_ids": data["input_ids"],
                "attention_mask": data["attention_mask"]
            }
        return None


# ==========================
# é‡å­åŒ–
# ==========================
def quantize_model(tokenizer, onnx_fp32: str, output_dir: str):
    print("\n[4] Executing INT8 quantization...")
    onnx_int8 = os.path.join(output_dir, "model_int8.onnx")
    quantize_static(
        model_input=onnx_fp32,
        model_output=onnx_int8,
        calibration_data_reader=DummyCalibReader(tokenizer),
        quant_format=QuantType.QUInt8
    )
    print(f"âœ… INT8 quantization complete â†’ {onnx_int8}")


# ==========================
# GGUFã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (ğŸš¨ æ–°è¦è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³)
# ==========================
def export_gguf(output_dir: str):
    print("\n[5] Exporting GGUF (for bert.cpp/llama.cpp)...")
    
    # GGUFå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ(convert-to-ggml.pyãªã©)ã®ãƒ‘ã‚¹ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    # Dockerfileã§ GGUF_CONVERT_SCRIPT=/app/bert.cpp/models/convert-to-ggml.py ã®ã‚ˆã†ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æœŸå¾…
    convert_script_path = os.environ.get("GGUF_CONVERT_SCRIPT")
    
    if not convert_script_path or not os.path.exists(convert_script_path):
        print(f"  âš ï¸ WARNING: GGUF_CONVERT_SCRIPT environment variable not set or path invalid. Skipping GGUF export.")
        print(f"  (Path checked: {convert_script_path})")
        print("\nğŸ¯ All training and ONNX export processes completed (GGUF skipped).")
        return # GGUFä»¥å¤–ã¯æˆåŠŸã¨ã—ã¦çµ‚äº†

    # bert.cpp/models/convert-to-ggml.py ã®å‘½åè¦å‰‡ã«åˆã‚ã›ã‚‹
    gguf_output_path = os.path.join(output_dir, f"ggml-model-f16.gguf") 
    
    # ğŸš¨ ä¿®æ­£: å¤‰æ›ã‚³ãƒãƒ³ãƒ‰ã®å¼•æ•°ã®é †åºã‚’ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã«åŸºã¥ãå¤‰æ›´
    # å‘¼ã³å‡ºã—å½¢å¼: python convert-to-ggml.py <model_dir> <ftype_int> <output_file>
    # ftype_int: 0=f32, 1=f16 (ã¨æƒ³å®š)
    cmd = [
        sys.executable,         # ç¾åœ¨ã®Pythonã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿
        convert_script_path,
        output_dir,             # sys.argv[1]: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        "1",                    # sys.argv[2]: å‡ºåŠ›ã‚¿ã‚¤ãƒ— (1 = f16 ã¨æƒ³å®š)
        gguf_output_path        # sys.argv[3]: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    ]
    
    print(f"  Running GGUF conversion command: {' '.join(cmd)}")
    
    try:
        # ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã®æ¨™æº–å‡ºåŠ›ã¨ã‚¨ãƒ©ãƒ¼ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã—ã¤ã¤å®Ÿè¡Œ
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8')
        
        if process.stdout:
            for line in iter(process.stdout.readline, ''):
                print(f"  [GGUF]: {line.strip()}", flush=True)
            process.stdout.close()
        
        return_code = process.wait()
        
        if return_code != 0:
            raise subprocess.CalledProcessError(return_code, cmd)
            
        print(f"âœ… GGUF export complete â†’ {gguf_output_path}")
        print("\nğŸ¯ All training and export processes completed (including GGUF).")
        
    except subprocess.CalledProcessError as e:
        print(f"  âŒ ERROR: GGUF conversion failed with return code {e.returncode}.", file=sys.stderr)
        print("\nğŸ¯ Training and ONNX export completed (GGUF FAILED).")
        # GGUFå¤‰æ›ãŒå¤±æ•—ã—ã¦ã‚‚ã€ONNXã¯æˆåŠŸã—ã¦ã„ã‚‹ã®ã§ã€ã“ã“ã§ã¯å‡¦ç†ã‚’åœæ­¢ã—ãªã„
        
    except Exception as e:
        print(f"  âŒ ERROR: An unexpected error occurred during GGUF conversion: {e}", file=sys.stderr)
        print("\nğŸ¯ Training and ONNX export completed (GGUF FAILED).")


# ==========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç† (éå¯¾è©±å‹)
# ==========================
def main():
    parser = argparse.ArgumentParser(description="Fine-tuning and model export script for TinyBERT models.")
    # ğŸš¨ ä¿®æ­£: .add.argument ã‚’ .add_argument ã«ä¿®æ­£
    parser.add_argument("--base_model_path", required=True, help="Local path to the base model directory (e.g., /app/worker/.../bert-tiny).")
    parser.add_argument("--training_file", required=True, help="Local path to the training data file (e.g., /tmp/job_ID/data/train_triplets.txt).")
    parser.add_argument("--output_dir", required=True, help="Directory to save the fine-tuned model and exports.")
    # ğŸš¨ ä¿®æ­£: æŠœã‘ã¦ã„ãŸ --epochs ã¨ --lr ã®å¼•æ•°ã‚’è¿½åŠ 
    parser.add_argument("--epochs", type=int, default=EPOCHS, help=f"Number of training epochs (default: {EPOCHS}).")
    parser.add_argument("--lr", type=float, default=LR, help=f"Learning rate (default: {LR}).")

    args = parser.parse_args()

    # --- å‰å‡¦ç† ---
    if not os.path.exists(args.training_file):
        raise FileNotFoundError(f"Training file not found: {args.training_file}")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    os.makedirs(args.output_dir, exist_ok=True)

    # --- è¨“ç·´å®Ÿè¡Œ ---
    tokenizer, model = finetune_model(
        model_name_or_path=args.base_model_path,
        training_file=args.training_file,
        output_dir=args.output_dir,
        epochs=args.epochs,
        lr=args.lr
    )

    # --- ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¨é‡å­åŒ– ---
    onnx_fp32 = export_onnx(model, tokenizer, args.output_dir)
    quantize_model(tokenizer, onnx_fp32, args.output_dir)

    # --- GGUFã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (ğŸš¨ ç·¨é›†ç®‡æ‰€) ---
    export_gguf(args.output_dir)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nFATAL: Training pipeline failed: {e}", file=sys.stderr)
        sys.exit(1) # ãƒ¯ãƒ¼ã‚«ãƒ¼ã«å¤±æ•—ã‚’é€šçŸ¥

