#!/usr/bin/env python3
"""
train_and_export.py

ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€INT8é‡å­åŒ–ã€GGUFã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Ÿè¡Œç’°å¢ƒã§éå¯¾è©±çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
"""

from __future__ import annotations
import os
import argparse
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType
import numpy as np
from tqdm import tqdm
import sys
import subprocess
import glob # GGUFã®quantizeãƒã‚¤ãƒŠãƒªæ¤œç´¢ã®ãŸã‚ã«è¿½åŠ 

# ==========================
# å…±é€šè¨­å®š (å¼•æ•°ã§ä¸Šæ›¸ãã•ã‚Œãªã„é™ã‚Šã“ã®å€¤ã‚’ä½¿ç”¨)
# ==========================
MAX_LENGTH = 32
BATCH_SIZE = 16
EPOCHS = 3
LR = 2e-5
# Dockerç’°å¢ƒã§ã¯CUDAãŒåˆ©ç”¨å¯èƒ½ã‹ä¸æ˜ãªãŸã‚ã€CPUã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã€CUDAãŒã‚ã‚Œã°ä½¿ç”¨
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TRAIN_DATA_DEFAULT = "data/train_triplets.txt"


# ==========================
# SBERTæ§‹é€ ï¼ˆmean poolingï¼‰
# ==========================
class SBERTEncoder(nn.Module):
    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model

    def forward(self, input_ids, attention_mask):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden = output.last_hidden_state
        # Mean Poolingã®å‡¦ç†
        mask = attention_mask.unsqueeze(-1).expand(last_hidden.size()).float()
        summed = (last_hidden * mask).sum(1)
        # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹ãŸã‚ã«min=1e-9ã‚’è¨­å®š
        counts = torch.clamp(mask.sum(1), min=1e-9) 
        mean_pooled = summed / counts
        return mean_pooled


# ==========================
# Tripletãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
# ==========================
class TripletDataset(Dataset):
    # ä¿®æ­£: MAX_LENGTHã‚’å¼•æ•°ã¨ã—ã¦å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
    def __init__(self, path: str, tokenizer, max_length: int):
        self.samples = []
        try:
            with open(path, encoding="utf-8") as f:
                for line in f:
                    parts = line.strip().split("\t")
                    if len(parts) == 3:
                        self.samples.append(tuple(parts))
        except FileNotFoundError:
             print(f"ERROR: Training data file not found at {path}", file=sys.stderr)
             raise
        except Exception as e:
             print(f"ERROR: Failed to read training data: {e}", file=sys.stderr)
             raise

        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        a, p, n = self.samples[idx]
        return self.tokenizer(
            [a, p, n],
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=self.max_length # ä¿®æ­£: ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã‚’ä½¿ç”¨
        )


# ==========================
# Triplet Loss
# ==========================
def triplet_loss(anchor, positive, negative, margin=1.0):
    d_ap = (anchor - positive).pow(2).sum(1)
    d_an = (anchor - negative).pow(2).sum(1)
    return torch.relu(d_ap - d_an + margin).mean()


# ==========================
# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†
# ==========================
# ä¿®æ­£: max_lengthã‚’å¼•æ•°ã«è¿½åŠ 
def finetune_model(model_name_or_path: str, training_file: str, output_dir: str, epochs: int, lr: float, max_length: int):
    print(f"\n[1] Loading model from {model_name_or_path} and tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    base_model = AutoModel.from_pretrained(model_name_or_path)
    model = SBERTEncoder(base_model).to(DEVICE)

    # ä¿®æ­£: max_lengthã‚’æ¸¡ã™
    dataset = TripletDataset(training_file, tokenizer, max_length) 
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    
    # ONNX INT8é‡å­åŒ–ã®ãŸã‚ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾— (è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®å‰ã«å–å¾—)
    calib_data_batch = next(iter(dataloader)) 

    print(f"[2] Starting fine-tuning (Epochs: {epochs}, LR: {lr})...")
    model.train()
    for epoch in range(epochs):
        losses = []
        for batch in tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}"):
            input_ids = batch["input_ids"].squeeze(1).to(DEVICE)
            attention_mask = batch["attention_mask"].squeeze(1).to(DEVICE)
            a, p, n = input_ids[:, 0, :], input_ids[:, 1, :], input_ids[:, 2, :]
            am, pm, nm = attention_mask[:, 0, :], attention_mask[:, 1, :], attention_mask[:, 2, :]
            
            va, vp, vn = model(a, am), model(p, pm), model(n, nm)
            loss = triplet_loss(va, vp, vn)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            losses.append(loss.item())

        print(f"  âœ… Epoch {epoch+1}/{epochs} Average Loss: {np.mean(losses):.4f}")

    print("\nâœ… Fine-tuning complete.")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    model.bert.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    torch.save(model.state_dict(), os.path.join(output_dir, "pytorch_model.bin"))
    print(f"âœ… Model saved to {output_dir}")
    # ä¿®æ­£: ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã®ãŸã‚ã€å…ƒã®BERTãƒ¢ãƒ‡ãƒ«ã¨ã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã‚’è¿”ã™
    return tokenizer, model.bert, calib_data_batch


# ==========================
# ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# ==========================
# ä¿®æ­£: max_lengthã‚’å¼•æ•°ã«è¿½åŠ 
def export_onnx(model, tokenizer, output_dir: str, max_length: int):
    print("\n[3] Exporting ONNX (FP32)...")
    model.eval()
    onnx_fp32 = os.path.join(output_dir, "model_fp32.onnx")
    # ä¿®æ­£: max_lengthã‚’ä½¿ç”¨
    dummy_input_ids = torch.randint(0, tokenizer.vocab_size, (1, max_length), dtype=torch.long)
    dummy_attention_mask = torch.ones((1, max_length), dtype=torch.long)

    # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¯å…ƒã®BERTãƒ¢ãƒ‡ãƒ«ã§è¡Œã† (SBERTEncoderã§ã¯ãªã„)
    torch.onnx.export(
        model,
        (dummy_input_ids, dummy_attention_mask),
        onnx_fp32,
        input_names=["input_ids", "attention_mask"],
        output_names=["last_hidden_state"],
        dynamic_axes={"input_ids": {0: "batch"}, "attention_mask": {0: "batch"}},
        opset_version=14 # ä¿®æ­£æ¸ˆã¿
    )
    print(f"âœ… ONNX FP32 output complete â†’ {onnx_fp32}")
    return onnx_fp32


# ==========================
# ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
# ==========================
class CalibDataReaderFromBatch(CalibrationDataReader):
    # ä¿®æ­£: å®Ÿéš›ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã—ãŸãƒãƒƒãƒã‚’ä½¿ç”¨ã™ã‚‹ã‚¯ãƒ©ã‚¹
    def __init__(self, calib_data_batch):
        # ãƒãƒƒãƒã‹ã‚‰ã€æœ€åˆã®Anchoræ–‡ã® input_ids ã¨ attention_mask ã‚’æŠ½å‡º
        input_ids = calib_data_batch["input_ids"].squeeze(1)[:, 0, :]
        attention_mask = calib_data_batch["attention_mask"].squeeze(1)[:, 0, :]
        
        # NumPyé…åˆ—ã«å¤‰æ› (ONNX Runtimeã®è¦æ±‚)
        self.input_ids = input_ids.numpy()
        self.attention_mask = attention_mask.numpy()
        self.index = 0

    def get_next(self):
        if self.index < len(self.input_ids):
            data = {
                # ONNX RuntimeãŒæœŸå¾…ã™ã‚‹å½¢çŠ¶ (1, max_length) ã«reshape
                "input_ids": self.input_ids[self.index].reshape(1, -1),
                "attention_mask": self.attention_mask[self.index].reshape(1, -1)
            }
            self.index += 1
            return data
        return None


# ==========================
# é‡å­åŒ–
# ==========================
# ä¿®æ­£: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã‚’å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
def quantize_model(calib_data_batch, onnx_fp32: str, output_dir: str):
    print("\n[4] Executing INT8 quantization...")
    onnx_int8 = os.path.join(output_dir, "model_int8.onnx")
    
    # ä¿®æ­£: å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰CalibDataReaderã‚’åˆæœŸåŒ–
    calib_reader = CalibDataReaderFromBatch(calib_data_batch)
    
    quantize_static(
        model_input=onnx_fp32,
        model_output=onnx_int8,
        calibration_data_reader=calib_reader,
        quant_format=QuantType.QUInt8
    )
    print(f"âœ… INT8 quantization complete â†’ {onnx_int8}")


# ==========================
# GGUFã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (llama.cppå¯¾å¿œã«æ›¸ãæ›ãˆæ¸ˆã¿)
# ==========================
def export_gguf(output_dir: str):
    print("\n[5] Exporting GGUF (using llama.cpp conversion tools)...")
    
    # 1. ãƒ‘ã‚¹è§£æ±º: ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‘ã‚¹ã‚’ä½¿ç”¨
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    convert_script_path = os.environ.get("GGUF_CONVERT_SCRIPT")
    quantize_script_path = os.environ.get("GGUF_QUANTIZE_SCRIPT")

    # ğŸš¨ ä¿®æ­£: ç’°å¢ƒå¤‰æ•°ã§å–å¾—ã§ããªã‹ã£ãŸå ´åˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‘ã‚¹ã§ä¸Šæ›¸ãã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
    if not os.path.exists(convert_script_path) or not os.path.exists(quantize_script_path):
        LLAMA_CPP_BASE = "/app/llama.cpp"
        # ç’°å¢ƒå¤‰æ•°ãŒãªã„ã€ã¾ãŸã¯ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã€æ—¢çŸ¥ã®ã‚¯ãƒ­ãƒ¼ãƒ³ãƒ‘ã‚¹ã‚’è©¦ã™
        convert_script_path = os.path.join(LLAMA_CPP_BASE, "convert_hf_to_gguf.py")
        quantize_script_path = os.path.join(LLAMA_CPP_BASE, "build/bin/quantize") 

    
    if not os.path.exists(convert_script_path):
        print(f"  âŒ ERROR: Conversion script not found. Skipping GGUF export.")
        print(f"  (Checked path: {convert_script_path})")
        print("---")
        return

    # 1. F16 (Full Precision) ã¸ã®å¤‰æ› (llama.cpp/convert-hf-to-gguf.pyã‚’ä½¿ç”¨)
    gguf_f16_path = os.path.join(output_dir, "ggml-model-f16.gguf")
    
    # å‘¼ã³å‡ºã—å½¢å¼: python convert-hf-to-gguf.py <model_dir> --outfile <output_file> --outtype f16
    cmd_f16 = [
        sys.executable,
        convert_script_path,
        output_dir, 
        "--outfile", gguf_f16_path,
        "--outtype", "f16"
    ]
    
    print(f"  [5-1] Running F16 GGUF conversion: {' '.join(cmd_f16)}")
    
    try:
        # F16å¤‰æ›ã‚’å®Ÿè¡Œ
        # capture_output=Falseã«ã—ã¦ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å‡ºåŠ›ãŒè¦‹ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ (ãƒ‡ãƒãƒƒã‚°ç”¨é€”)
        subprocess.run(cmd_f16, check=True, text=True, encoding='utf-8')
        print(f"  âœ… F16 GGUF export complete â†’ {gguf_f16_path}")
        
    except subprocess.CalledProcessError as e:
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        stderr_output = e.stderr.decode('utf-8') if isinstance(e.stderr, bytes) else e.stderr
        print(f"  âŒ ERROR: F16 GGUF conversion failed with return code {e.returncode}.", file=sys.stderr)
        print(f"  --- Stderr ---\n{stderr_output}", file=sys.stderr)
        print("---")
        return


    # 2. Q4_0 (4-bit quantization) ã¸ã®é‡å­åŒ– (llama.cpp/quantizeãƒã‚¤ãƒŠãƒªã‚’ä½¿ç”¨)
    if not os.path.exists(quantize_script_path):
        print("  âš ï¸ WARNING: GGUF quantize binary not found. Skipping 4-bit quantization.")
        print(f"  (Checked path: {quantize_script_path})")
        print("\nğŸ¯ All training and export processes completed (GGUF Q4_0 skipped).")
        return
        
    gguf_q4_path = os.path.join(output_dir, "ggml-model-q4_0.gguf") 
    
    # å‘¼ã³å‡ºã—å½¢å¼: ./quantize <input.gguf> <output.gguf> <quant_type>
    cmd_q4 = [
        quantize_script_path,
        gguf_f16_path,          # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ« (F16ãƒ¢ãƒ‡ãƒ«)
        gguf_q4_path,           # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« (Q4_0ãƒ¢ãƒ‡ãƒ«)
        "Q4_0"                  # é‡å­åŒ–ã‚¿ã‚¤ãƒ—
    ]
    
    print(f"  [5-2] Running Q4_0 quantization: {' '.join(cmd_q4)}")
    
    try:
        # Q4_0é‡å­åŒ–ã‚’å®Ÿè¡Œ
        # capture_output=Falseã«ã—ã¦ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å‡ºåŠ›ãŒè¦‹ãˆã‚‹ã‚ˆã†ã«ã™ã‚‹
        subprocess.run(cmd_q4, check=True, text=True, encoding='utf-8')
        print(f"  âœ… Q4_0 GGUF export complete â†’ {gguf_q4_path}")
        
    except subprocess.CalledProcessError as e:
        # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        stderr_output = e.stderr.decode('utf-8') if isinstance(e.stderr, bytes) else e.stderr
        print(f"  âŒ ERROR: Q4_0 GGUF quantization failed with return code {e.returncode}.", file=sys.stderr)
        print(f"  --- Stderr ---\n{stderr_output}", file=sys.stderr)
        print("\nğŸ¯ Training and ONNX export completed (GGUF Q4_0 FAILED).")
        return
        
    print("\nğŸ¯ All training and export processes completed (including GGUF).")


# ==========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç† (éå¯¾è©±å‹)
# ==========================
def main():
    parser = argparse.ArgumentParser(description="Fine-tuning and model export script for TinyBERT models.")
    # ä¿®æ­£: .add_argument ã«çµ±ä¸€
    parser.add_argument("--base_model_path", required=True, help="Local path to the base model directory (e.g., /app/worker/.../bert-tiny).")
    parser.add_argument("--training_file", required=True, help="Local path to the training data file (e.g., /tmp/job_ID/data/train_triplets.txt).")
    parser.add_argument("--output_dir", required=True, help="Directory to save the fine-tuned model and exports.")
    # ä¿®æ­£: æŠœã‘ã¦ã„ãŸå¼•æ•°ã‚’è¿½åŠ 
    parser.add_argument("--epochs", type=int, default=EPOCHS, help=f"Number of training epochs (default: {EPOCHS}).")
    parser.add_argument("--lr", type=float, default=LR, help=f"Learning rate (default: {LR}).")
    # è¿½åŠ : MAX_LENGTHã‚‚å¼•æ•°ã§å—ã‘å–ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
    parser.add_argument("--max_length", type=int, default=MAX_LENGTH, help=f"Maximum sequence length (default: {MAX_LENGTH}).")

    args = parser.parse_args()

    # --- å‰å‡¦ç† ---
    if not os.path.exists(args.training_file):
        raise FileNotFoundError(f"Training file not found: {args.training_file}")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    os.makedirs(args.output_dir, exist_ok=True)

    # --- è¨“ç·´å®Ÿè¡Œ ---
    # ä¿®æ­£: max_lengthã‚’æ¸¡ã—ã€calib_data_batchã‚’å—ã‘å–ã‚‹
    tokenizer, model, calib_data_batch = finetune_model(
        model_name_or_path=args.base_model_path,
        training_file=args.training_file,
        output_dir=args.output_dir,
        epochs=args.epochs,
        lr=args.lr,
        max_length=args.max_length
    )

    # --- ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã¨é‡å­åŒ– ---
    # ä¿®æ­£: max_lengthã‚’æ¸¡ã™
    onnx_fp32 = export_onnx(model, tokenizer, args.output_dir, args.max_length)
    # ä¿®æ­£: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™
    quantize_model(calib_data_batch, onnx_fp32, args.output_dir)

    # --- GGUFã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (æœ€çµ‚ç›®çš„) ---
    export_gguf(args.output_dir)


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        import traceback
        traceback.print_exc(file=sys.stderr)
        print(f"\nFATAL: Training pipeline failed: {e}", file=sys.stderr)
        sys.exit(1)
